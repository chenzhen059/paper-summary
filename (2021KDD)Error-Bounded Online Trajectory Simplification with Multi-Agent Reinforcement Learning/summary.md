The paper uses Error-Bounded and dropping some sampled points to subtract the computation and ratio. This algorithm called MARL4TS involves two agents for different decision. Agent-E is to expand the number of the window using the RL(reinforcement learning) to compute the step length. Agent-R is to re-open a window when the error exceeds the threshold we set. This paper also utilize MDP(Markov decision process) to produce proper parameters and SED(synchronous Euclidean distance), PED(perpendicular Euclidean distance), DAD(direction-based distance), SAD(speed-based distance) to measure the loss, and adopt the DQN(Deep-Q-Network) method for learning the policies for Agent-E and Agent-R.
1.	The MDP involves four steps including states, actions, transitions, rewards, and the future is independent of the past given the present. Agent connects with environment all the time in RL, so agent will take an action when it receives the state from the environment. After that, agent can get a reward to become a new state.
2.	The algorithm MARL4TS is inputted a trajectory T, and outputs a simplified trajectory T'. To begin with, we set a window[1,2]. When the loss is included in threshold, we expand the windows by receiving parameter from agent-E. We will re-open a window, however, when error exceeds the threshold until the window comes end.
EB-OTS can drop as many points as possible to optimalize trajectory to keep the efficiency and effectiveness. Using this algorithm lower compression ratios and runs comparably fast.
Experiment: 1.Baseline methods 2.Change parameters.
